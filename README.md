# Exploring the transformation of neural networks

It aims to implement the ideas of each stage of the neural network with the simplest possible architecture.

## First stop: The dawn of the perceptron - the first step of the machine
Imagine you are in front of a huge blank canvas with only a pen. Frank Rosenblatt gave the pen life in 1957, creating the Perceptron. It is like a child who has learned to draw lines with a pen and can distinguish between straight lines and curves, but cannot yet fully draw complex pictures.
## Second stop: Multi-layer feedforward network and backpropagation - building complex pictures
As the years progressed into the 1980s, scientists seemed to add multiple pens and tools for color and canvas changes. Whenever something doesn't look right in a painting, the painter (the network) takes a step back and adjusts the brushstrokes and colors (weights and biases) to more accurately represent the imagined picture. In other words, the emergence of multi-layer perceptrons and backpropagation algorithms allow neural networks to not only do simple classification, but can even simulate more functions.
## Third Stop: Convolutional Neural Networks and LeNet’s Visual Revolution
Come 1989, Yann LeCun seems to have invented a new brush, especially suitable for painting complex portraits and richly detailed backgrounds. Convolutional neural networks (CNN) began to sprout at this time, and LeNet's article used handwritten data sets as the starting point for implementation.
## Fourth stop: Breakthroughs in deep learning and AlexNet - describing a new dimension
In the 2010s, it seemed like the entire studio had been upgraded, with increased canvas sizes and customization of brushes. AlexNet not only inherits the brushwork of its predecessors, but also adds powerful tools (GPU acceleration) to make the details and colors of the paintings richer, and it can be painted on a larger canvas. This model shined in the large-scale visual recognition challenge, which included more than 20,000 categories, completely changing the original simple neural network architecture.
## Stop 5: Generative Adversarial Networks and Variational Autoencoders – Artists’ Dream Tools
By the mid-to-late 2010s, scientists had access to fantastic new tools: GANs and VAEs. Taking painting as an example again, it's like they can create colors and shapes they've never seen before, or even dream-like scenes. This model drives neural networks that possess creativity.
## Stop Six: Self-Attention and Transformer Model - Creating Painting Friends
Finally, in 2017, the emergence of the Transformer model is like creating a painter. You only need to tell the painter which tools to use to draw what style. At this time, it is like having an extra painting friend. This model opens the door to generative AI.
